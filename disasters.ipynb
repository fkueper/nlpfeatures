{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggel: Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is obtained from a Kaggel challenge to detect tweets referring to a disaster:\n",
    "https://www.kaggle.com/c/nlp-getting-started/overview\n",
    "\n",
    "The notebooks' goal is to derive features from natural language transcripts which could be subsequently used in statistical methods.\n",
    "\n",
    "The notebook conducts the following steps:\n",
    "\n",
    "    1) Import libraries and data\n",
    "    2) Define funcitons for data preparation\n",
    "    3) Prepare data\n",
    "    4) Apply ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "train = pd.read_csv('nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'keyword', 'location', 'text', 'target']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.tolist() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id      target\n",
      "count   7613.000000  7613.00000\n",
      "mean    5441.934848     0.42966\n",
      "std     3137.116090     0.49506\n",
      "min        1.000000     0.00000\n",
      "25%     2734.000000     0.00000\n",
      "50%     5408.000000     0.00000\n",
      "75%     8146.000000     1.00000\n",
      "max    10873.000000     1.00000\n"
     ]
    }
   ],
   "source": [
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The sample is slightly imbalanced with 57% of tweets about real disasters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge test and train\n",
    "dfs = [train, test]\n",
    "df = pd.concat(dfs)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "\n",
      "Forest fire near La Ronge Sask. Canada\n",
      "\n",
      "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "\n",
      "13,000 people receive #wildfires evacuation orders in California \n",
      "\n",
      "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
      "\n",
      "#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
      "\n",
      "#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\n",
      "\n",
      "I'm on top of the hill and I can see a fire in the woods...\n",
      "\n",
      "There's an emergency evacuation happening now in the building across the street\n",
      "\n",
      "I'm afraid that the tornado is coming to our area...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print examples of tweets \n",
    "\n",
    "loop = np.arange(10)\n",
    "\n",
    "for i in loop:\n",
    "    print(train.loc[train.target == 1].reset_index().text[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's up man?\n",
      "\n",
      "I love fruits\n",
      "\n",
      "Summer is lovely\n",
      "\n",
      "My car is so fast\n",
      "\n",
      "What a goooooooaaaaaal!!!!!!\n",
      "\n",
      "this is ridiculous....\n",
      "\n",
      "London is cool ;)\n",
      "\n",
      "Love skiing\n",
      "\n",
      "What a wonderful day!\n",
      "\n",
      "LOOOOOOL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# non disaster tweets \n",
    "\n",
    "for i in loop:\n",
    "    print(train.loc[train.target == 0].reset_index().text[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The displayed samples show us that the documents contain words carrying little meaning in itself (e.g., 'are', 'the', 'by'), special characters, or numbers.\n",
    "    \n",
    "While some of these aspects might not support the analysis on its own, they could be used to derive other features to facilitate our task.\n",
    "For instance, articles and numbers might describe concrete events indicating a real disaster while certain special characters could hint towards urgency.\n",
    "\n",
    "Thus, before removing this potential information, variables are derived to capture some of the assumed indicatiors:\n",
    "   \n",
    "    1) Concrete language: use or articles ('a','an',etc.), prepositions ('to','with',etc.), and quantifiers (numbers, 'many', 'few', etc.) might indicate an higly contextualized tweet. \n",
    "    2) Excitement: Exclamation marks might indicate exciting situations \n",
    "    \n",
    "These categories could be captured using more advanced psycholingusitcal software such as LIWC or DICTON. For the purpose of this notebook, I chose to use a simple count algorithm to find the respective word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Define functions for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary holding wordcounts. \n",
    "\n",
    "def dictionary_empty(wordlist):\n",
    "    \"\"\"\n",
    "    Creates an empty dictionary containing words specified in the input list.\n",
    "    Input: List of words\n",
    "    Return: Empty dictionary \n",
    "    \"\"\"\n",
    "    di = dict()\n",
    "    for w in wordlist:               # create dictionary containing specified words\n",
    "        if w in di:\n",
    "            continue\n",
    "        else:\n",
    "            di[w] = 0\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the words of a wordlist\n",
    "def count_words(wordlist, corpus):\n",
    "    \"\"\"\n",
    "    Searches corpus for words in dictionary and returns a sum of these words for each document.\n",
    "    Input: wordlist with words, corpus of documents\n",
    "    Return: Dataframe with category word counts\n",
    "    \"\"\"\n",
    "    \n",
    "    di = dictionary_empty(wordlist)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df_append = []\n",
    "    for txt in corpus:\n",
    "        wds = txt.split()\n",
    "        for w in wds:\n",
    "            if w in di:\n",
    "                di[w] += 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        wds_count = sum(di.values())\n",
    "        df_wds = pd.DataFrame(np.array([wds_count]))\n",
    "        df_append.append(df_wds)\n",
    "\n",
    "        di = dictionary_empty(wordlist)\n",
    "        \n",
    "    df = pd.concat(df_append, axis = 0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean corpus\n",
    "\n",
    "def clean_txt(corpus):\n",
    "    \"\"\"\n",
    "    This function takes a string of textual data and returns a cleaned corpus.\n",
    "    Steps include: \n",
    "    Remove words with less than three letters. \n",
    "    Convert letters to lowercase\n",
    "    Remove stop words(nltk) \n",
    "    Remove special characters\n",
    "    \n",
    "    Inputs: Series with text strings\n",
    "    Returns: Series with cleaned text strings\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # rm special characters\n",
    "    df['clean'] = corpus.str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    # lower case \n",
    "    df['clean'] = df['clean'].apply(lambda x: x.lower())\n",
    "    # rm words with less than 3 letters\n",
    "    df['clean'] = df['clean'].fillna('').apply(lambda x: ' '.join([w for w in x.split() if len(w) >2]))\n",
    "    # stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    # tokenize \n",
    "    tokenized = df['clean'].fillna('').apply(lambda x: x.split())\n",
    "    # rm stop words\n",
    "    tokenized = tokenized.apply(lambda x: [w for w in x if w not in stop_words])\n",
    "    # de-tokenize\n",
    "    detokenized = []\n",
    "    for i in range(len(df)):\n",
    "        t = ' '.join(tokenized[i])\n",
    "        detokenized.append(t)\n",
    "    df['clean'] = detokenized\n",
    "\n",
    "    # define dfclean as corpus\n",
    "    corpus = df['clean']\n",
    "    \n",
    "    return corpus    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries\n",
    "\n",
    "words_concrete = 'a an the to with on in at for from by some many few all most more less'\n",
    "words_concrete = words_concrete.split() # split to create a list\n",
    "\n",
    "words_excitement = '! '\n",
    "words_excitement = words_excitement.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define corpus\n",
    "\n",
    "corpus = df.text\n",
    "\n",
    "# create dictionary counts\n",
    "di_concrete = count_words(words_concrete, corpus)\n",
    "di_excitement = count_words(words_excitement, corpus)\n",
    "\n",
    "# create clean corpus\n",
    "corpus_clean = clean_txt(corpus)\n",
    "\n",
    "# create tfidf tokens\n",
    "vectorizer = TfidfVectorizer(max_features=2000, # consider only top N features          \n",
    "                             max_df=0.9)        # ignore terms with a document frequency >0.7\n",
    "tfidf = vectorizer.fit_transform(corpus_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate topic model\n",
    "\n",
    "svd = TruncatedSVD(n_components=3,\n",
    "                   algorithm='randomized',\n",
    "                   n_iter=100,\n",
    "                   random_state=37)\n",
    "lsa = svd.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: \n",
      "http\n",
      "\n",
      "via\n",
      "\n",
      "fire\n",
      "\n",
      "new\n",
      "\n",
      "news\n",
      "\n",
      "Topic 2: \n",
      "https\n",
      "\n",
      "like\n",
      "\n",
      "amp\n",
      "\n",
      "fire\n",
      "\n",
      "burning\n",
      "\n",
      "Topic 3: \n",
      "https\n",
      "\n",
      "http\n",
      "\n",
      "via\n",
      "\n",
      "youtube\n",
      "\n",
      "disaster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check topics \n",
    "\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd.components_):\n",
    "  terms_comp = zip(vocabulary, comp)\n",
    "  sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:5]\n",
    "  print(\"Topic \"+str(i +1)+\": \")\n",
    "  for t in sorted_terms:\n",
    "    print(t[0])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Apply ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge features\n",
    "\n",
    "X = pd.DataFrame(lsa, \n",
    "                 columns=['topic_1', 'topic_2','topic_3'])\n",
    "X['concrete'] = di_concrete\n",
    "X['excitement'] = di_excitement\n",
    "\n",
    "X_train = X.loc[:len(train)-1]\n",
    "y_train = df['target'].loc[:len(train)-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear', C=1, random_state=37)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Wrap-up\n",
    "\n",
    "This notebook takes tweets published in the \"Disaster Tweet\" challenge on Kaggle to apply feature engineering on natural language.\n",
    "\n",
    "Word counts, TFIDF-vectorization, and latent semantic analysis were applied to derive variables which could be used as input variables for machine learning algorithms. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
